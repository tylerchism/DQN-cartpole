{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled4.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tylerchism/DQN-cartpole/blob/master/cartpole.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A7AwOHPMALE4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "outputId": "d6adfbe6-9307-42d1-a60e-be7432cbc120"
      },
      "source": [
        "pip install scikit-optimize"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting scikit-optimize\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f4/44/60f82c97d1caa98752c7da2c1681cab5c7a390a0fdd3a55fac672b321cac/scikit_optimize-0.5.2-py2.py3-none-any.whl (74kB)\n",
            "\r\u001b[K     |████▍                           | 10kB 23.9MB/s eta 0:00:01\r\u001b[K     |████████▉                       | 20kB 1.8MB/s eta 0:00:01\r\u001b[K     |█████████████▏                  | 30kB 2.6MB/s eta 0:00:01\r\u001b[K     |█████████████████▋              | 40kB 1.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 51kB 2.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▍     | 61kB 2.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 71kB 2.9MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 81kB 2.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy>=0.14.0 in /usr/local/lib/python3.6/dist-packages (from scikit-optimize) (1.3.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from scikit-optimize) (1.17.3)\n",
            "Requirement already satisfied: scikit-learn>=0.19.1 in /usr/local/lib/python3.6/dist-packages (from scikit-optimize) (0.21.3)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn>=0.19.1->scikit-optimize) (0.14.0)\n",
            "Installing collected packages: scikit-optimize\n",
            "Successfully installed scikit-optimize-0.5.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xS869adN-oYo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "a686f144-35d8-4eb1-ce33-c52d22deb920"
      },
      "source": [
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "import os\n",
        "import math\n",
        "from collections import deque\n",
        "from chainer import functions as F\n",
        "import skopt\n",
        "import bottleneck as bn\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/externals/joblib/__init__.py:15: DeprecationWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
            "  warnings.warn(msg, category=DeprecationWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-k5Nx3yd_oIY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def learn_to_balance(model_layers=3,\n",
        "                     model_layer_size=34,\n",
        "                     model_layer_taper=1.0490184900205404,\n",
        "                     model_activation_1=2,\n",
        "                     model_activation_2=2,\n",
        "                     model_activation_3=1,\n",
        "                     mm_omega=3.3475360424700393,\n",
        "                     batch_size=113,\n",
        "                     memory_size=10000,\n",
        "                     discount_factor=0.9725879085561605,\n",
        "                     epsilon=0.9985988622823487,\n",
        "                     epsilon_decay=0.9469565368958073,\n",
        "                     lr=0.000584644333009868,\n",
        "                     epsilon_min=0.01,\n",
        "                     train_start=256,\n",
        "                     n_episodes=500,\n",
        "                     n_win_ticks=195,\n",
        "                     n_avg_scores=100,\n",
        "                     n_max_steps=200,\n",
        "                     logging_int=10,\n",
        "                     verbose=False,\n",
        "                     render=False,\n",
        "                     return_history=False,\n",
        "                     win_100_scalar=1.1, # Optimizer bonus for a fast win < 100 episodes\n",
        "                     win_10_scalar=1.15, # Optimizer bonus for a fast win < 10 episodes\n",
        "                     seed=123):\n",
        "\n",
        "  # Environment\n",
        "  import gym\n",
        "  env = gym.make('CartPole-v0')\n",
        "  state_size = env.observation_space.shape[0]\n",
        "  action_size = env.action_space.n\n",
        "\n",
        "  # Reproducibility\n",
        "  os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "  import random\n",
        "  import numpy as np\n",
        "  env.seed(seed)\n",
        "  env.action_space.seed(seed)\n",
        "  random.seed(seed)\n",
        "  np.random.seed(seed)\n",
        "\n",
        "  # Apply seed to tensorflow session\n",
        "  import tensorflow as tf\n",
        "  import keras.backend as K\n",
        "  from keras.utils.generic_utils import get_custom_objects\n",
        "  from keras.layers import Dense, Activation\n",
        "  from keras.models import Sequential\n",
        "  from keras.optimizers import Adam\n",
        "  tf.reset_default_graph()\n",
        "  session_conf = tf.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)\n",
        "  tf.set_random_seed(seed)\n",
        "  sess = tf.Session(graph=tf.get_default_graph(), config=session_conf)\n",
        "  K.set_session(sess)\n",
        "\n",
        "  # Mish Activation implementation\n",
        "  # Source: https://github.com/digantamisra98/Mish/blob/master/Mish/Keras/mish.py\n",
        "  class Mish(Activation):\n",
        "    def __init__(self, activation=\"Mish\", **kwargs):\n",
        "      super(Mish, self).__init__(activation, **kwargs)\n",
        "      self.__name__ = 'Mish'\n",
        "\n",
        "  def mish(x):\n",
        "    return x*K.tanh(K.softplus(x))\n",
        "\n",
        "  get_custom_objects().update({'Mish': Mish(mish)})\n",
        "\n",
        "  # Mellowmax: Softmax alternative\n",
        "  # See: http://arxiv.org/abs/1612.05628\n",
        "  # Source: https://github.com/chainer/chainerrl/blob/master/chainerrl/functions/mellowmax.py\n",
        "  def mellowmax(values, omega=1., axis=1):\n",
        "    n = values.shape[axis]\n",
        "    return (F.logsumexp(omega * values, axis=axis) - np.log(n)) / omega\n",
        "\n",
        "  # Build model\n",
        "  def build_model(state_size, action_size, model_layers=3, model_activation_1=2, model_activation_2=2, model_activation_3=2, model_layer_size=96, model_layer_taper=0.5, lr=0.003):\n",
        "    model = Sequential()\n",
        "\n",
        "    model.add(Dense(int(model_layer_size), input_dim=state_size, kernel_initializer='he_uniform'))\n",
        "\n",
        "    for i in range(int(model_layers)):\n",
        "      layer_size = min(16, int(model_layer_size * (model_layer_taper ** i)))\n",
        "      model.add(Dense(layer_size, kernel_initializer='he_uniform'))\n",
        "      if i % 3 == 0:\n",
        "        model_activation = model_activation_3\n",
        "      elif i % 2 == 0:\n",
        "        model_activation = model_activation_2\n",
        "      else:\n",
        "        model_activation = model_activation_1\n",
        "    \n",
        "      if round(model_activation) == 0:\n",
        "        model.add(Activation('relu'))\n",
        "      elif round(model_activation) == 1:\n",
        "        model.add(Activation('tanh'))\n",
        "      else:\n",
        "        model.add(Mish())\n",
        "    model.add(Dense(action_size, kernel_initializer='he_uniform'))\n",
        "    model.compile(Adam(lr=lr), loss='mse')\n",
        "    return model\n",
        "\n",
        "  # Training\n",
        "  # Source: https://github.com/yanpanlau/CartPole/blob/master/DQN/CartPole_DQN.py\n",
        "  def get_action(state, action_size, model, epsilon):\n",
        "    return np.random.randint(action_size) if np.random.rand() <= epsilon else np.argmax(model.predict(state)[0])\n",
        "\n",
        "  def train_replay(memory, batch_size, train_start, discount_factor, mm_omega, model):\n",
        "    if len(memory) < train_start:\n",
        "      return\n",
        "    minibatch = random.sample(memory,  min(int(batch_size), len(memory)))\n",
        "\n",
        "    # Experience replay\n",
        "    state_t, action_t, reward_t, state_t1, terminal = zip(*minibatch)\n",
        "    state_t = np.concatenate(state_t)\n",
        "    state_t1 = np.concatenate(state_t1)\n",
        "    targets = model.predict(state_t)\n",
        "    Q_sa = model.predict(state_t1)\n",
        "    mm = mellowmax(Q_sa, omega=mm_omega).data\n",
        "\n",
        "    targets[range(int(batch_size)), action_t] = reward_t + discount_factor * mm * np.invert(terminal)\n",
        "    model.train_on_batch(state_t, targets)\n",
        "\n",
        "  # Model\n",
        "  model = build_model(state_size, action_size,\n",
        "                      model_layers=model_layers, model_layer_size=model_layer_size,\n",
        "                      model_layer_taper=model_layer_taper, lr=lr)\n",
        "\n",
        "  # Training\n",
        "  solution = []\n",
        "  all_scores = []\n",
        "  scores = deque(maxlen=int(n_avg_scores))\n",
        "  memory = deque(maxlen=int(memory_size))\n",
        "  solution_window_start = n_episodes\n",
        "\n",
        "  for e in range(n_episodes):\n",
        "    done = False\n",
        "    score = 0\n",
        "    step = 0\n",
        "    state = env.reset()\n",
        "    state = np.reshape(state, [1, state_size])\n",
        "\n",
        "    while not done and step < n_max_steps:\n",
        "      action = get_action(state, action_size, model, epsilon)\n",
        "      next_state, reward, done, info = env.step(action)\n",
        "      next_state = np.reshape(next_state, [1, state_size])\n",
        "\n",
        "      memory.append(\n",
        "          (state, action, reward if not done else -100, next_state, done))\n",
        "      if epsilon > epsilon_min:\n",
        "        epsilon *= epsilon_decay  # Decrease randomness\n",
        "      train_replay(memory, batch_size, train_start, discount_factor, mm_omega, model)\n",
        "      score += reward\n",
        "      step += 1\n",
        "      state = next_state\n",
        "\n",
        "      if render:\n",
        "        env.render()\n",
        "\n",
        "      if done:\n",
        "        env.reset()\n",
        "        scores.append(score)\n",
        "        all_scores.append(score)\n",
        "        avg_score = np.mean(scores)\n",
        "\n",
        "        if len(solution) == 0 and avg_score >= n_win_ticks and e >= n_avg_scores:\n",
        "          # The start of a 100-episode averaging window with a mean score >= 195\n",
        "          solution_window_start = e - n_avg_scores\n",
        "\n",
        "          # The first episode score >= 195\n",
        "          solution_episode_idx = next(\n",
        "              x[0] for x in enumerate(all_scores) if x[1] >= n_win_ticks)\n",
        "\n",
        "          solution.append(solution_window_start)\n",
        "          print('Solved! Avg. reward >= 195.0 over 100 consecutive trials reached at episode {} \\o/'.format(\n",
        "              solution_window_start))\n",
        "          print('First score >= 195 reached at episode {}.'.format(\n",
        "              solution_episode_idx))\n",
        "\n",
        "        if verbose > 0 and e % logging_int == 0:\n",
        "          avg_display = '{:.2f}'.format(avg_score)\n",
        "          print('[Episode {}] Average Score: {} | Total Rewards: {:.2f}'.format(\n",
        "              e, avg_display, score))\n",
        "  return solution_window_start, np.mean(all_scores), all_scores\n",
        "\n",
        "def run_game(**config):\n",
        "  solution_window_start, avg_score, all_scores = learn_to_balance(**config)\n",
        "  if 'return_history' in config and config['return_history']:\n",
        "        return solution_window_start, avg_score, all_scores\n",
        "  if solution_window_start < 10:\n",
        "    return avg_score * config['win_10_scalar']\n",
        "  elif solution_window_start < 100:\n",
        "    return avg_score * config['win_100_scalar']\n",
        "  return avg_score"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8PQUaAsp_tyo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def learn_to_balance(model_layers=3,\n",
        "                     model_layer_size=34,\n",
        "                     model_layer_taper=1.0490184900205404,\n",
        "                     model_activation_1=2,\n",
        "                     model_activation_2=2,\n",
        "                     model_activation_3=1,\n",
        "                     mm_omega=3.3475360424700393,\n",
        "                     batch_size=113,\n",
        "                     memory_size=10000,\n",
        "                     discount_factor=0.9725879085561605,\n",
        "                     epsilon=0.9985988622823487,\n",
        "                     epsilon_decay=0.9469565368958073,\n",
        "                     lr=0.000584644333009868,\n",
        "                     epsilon_min=0.01,\n",
        "                     train_start=256,\n",
        "                     n_episodes=500,\n",
        "                     n_win_ticks=195,\n",
        "                     n_avg_scores=100,\n",
        "                     n_max_steps=200,\n",
        "                     logging_int=10,\n",
        "                     verbose=False,\n",
        "                     render=False,\n",
        "                     return_history=False,\n",
        "                     win_100_scalar=1.1, # Optimizer bonus for a fast win < 100 episodes\n",
        "                     win_10_scalar=1.15, # Optimizer bonus for a fast win < 10 episodes\n",
        "                     seed=123):\n",
        "\n",
        "  # Environment\n",
        "  import gym\n",
        "  env = gym.make('CartPole-v0')\n",
        "  state_size = env.observation_space.shape[0]\n",
        "  action_size = env.action_space.n\n",
        "\n",
        "  # Reproducibility\n",
        "  os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "  import random\n",
        "  import numpy as np\n",
        "  env.seed(seed)\n",
        "  env.action_space.seed(seed)\n",
        "  random.seed(seed)\n",
        "  np.random.seed(seed)\n",
        "\n",
        "  # Apply seed to tensorflow session\n",
        "  import tensorflow as tf\n",
        "  import keras.backend as K\n",
        "  from keras.utils.generic_utils import get_custom_objects\n",
        "  from keras.layers import Dense, Activation\n",
        "  from keras.models import Sequential\n",
        "  from keras.optimizers import Adam\n",
        "  tf.reset_default_graph()\n",
        "  session_conf = tf.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)\n",
        "  tf.set_random_seed(seed)\n",
        "  sess = tf.Session(graph=tf.get_default_graph(), config=session_conf)\n",
        "  K.set_session(sess)\n",
        "\n",
        "  # Mish Activation implementation\n",
        "  # Source: https://github.com/digantamisra98/Mish/blob/master/Mish/Keras/mish.py\n",
        "  class Mish(Activation):\n",
        "    def __init__(self, activation=\"Mish\", **kwargs):\n",
        "      super(Mish, self).__init__(activation, **kwargs)\n",
        "      self.__name__ = 'Mish'\n",
        "\n",
        "  def mish(x):\n",
        "    return x*K.tanh(K.softplus(x))\n",
        "\n",
        "  get_custom_objects().update({'Mish': Mish(mish)})\n",
        "\n",
        "  # Mellowmax: Softmax alternative\n",
        "  # See: http://arxiv.org/abs/1612.05628\n",
        "  # Source: https://github.com/chainer/chainerrl/blob/master/chainerrl/functions/mellowmax.py\n",
        "  def mellowmax(values, omega=1., axis=1):\n",
        "    n = values.shape[axis]\n",
        "    return (F.logsumexp(omega * values, axis=axis) - np.log(n)) / omega\n",
        "\n",
        "  # Build model\n",
        "  def build_model(state_size, action_size, model_layers=3, model_activation_1=2, model_activation_2=2, model_activation_3=2, model_layer_size=96, model_layer_taper=0.5, lr=0.003):\n",
        "    model = Sequential()\n",
        "\n",
        "    model.add(Dense(int(model_layer_size), input_dim=state_size, kernel_initializer='he_uniform'))\n",
        "\n",
        "    for i in range(int(model_layers)):\n",
        "      layer_size = min(16, int(model_layer_size * (model_layer_taper ** i)))\n",
        "      model.add(Dense(layer_size, kernel_initializer='he_uniform'))\n",
        "      if i % 3 == 0:\n",
        "        model_activation = model_activation_3\n",
        "      elif i % 2 == 0:\n",
        "        model_activation = model_activation_2\n",
        "      else:\n",
        "        model_activation = model_activation_1\n",
        "    \n",
        "      if round(model_activation) == 0:\n",
        "        model.add(Activation('relu'))\n",
        "      elif round(model_activation) == 1:\n",
        "        model.add(Activation('tanh'))\n",
        "      else:\n",
        "        model.add(Mish())\n",
        "    model.add(Dense(action_size, kernel_initializer='he_uniform'))\n",
        "    model.compile(Adam(lr=lr), loss='mse')\n",
        "    return model\n",
        "\n",
        "  # Training\n",
        "  # Source: https://github.com/yanpanlau/CartPole/blob/master/DQN/CartPole_DQN.py\n",
        "  def get_action(state, action_size, model, epsilon):\n",
        "    return np.random.randint(action_size) if np.random.rand() <= epsilon else np.argmax(model.predict(state)[0])\n",
        "\n",
        "  def train_replay(memory, batch_size, train_start, discount_factor, mm_omega, model):\n",
        "    if len(memory) < train_start:\n",
        "      return\n",
        "    minibatch = random.sample(memory,  min(int(batch_size), len(memory)))\n",
        "\n",
        "    # Experience replay\n",
        "    state_t, action_t, reward_t, state_t1, terminal = zip(*minibatch)\n",
        "    state_t = np.concatenate(state_t)\n",
        "    state_t1 = np.concatenate(state_t1)\n",
        "    targets = model.predict(state_t)\n",
        "    Q_sa = model.predict(state_t1)\n",
        "    mm = mellowmax(Q_sa, omega=mm_omega).data\n",
        "\n",
        "    targets[range(int(batch_size)), action_t] = reward_t + discount_factor * mm * np.invert(terminal)\n",
        "    model.train_on_batch(state_t, targets)\n",
        "\n",
        "  # Model\n",
        "  model = build_model(state_size, action_size,\n",
        "                      model_layers=model_layers, model_layer_size=model_layer_size,\n",
        "                      model_layer_taper=model_layer_taper, lr=lr)\n",
        "\n",
        "  # Training\n",
        "  solution = []\n",
        "  all_scores = []\n",
        "  scores = deque(maxlen=int(n_avg_scores))\n",
        "  memory = deque(maxlen=int(memory_size))\n",
        "  solution_window_start = n_episodes\n",
        "\n",
        "  for e in range(n_episodes):\n",
        "    done = False\n",
        "    score = 0\n",
        "    step = 0\n",
        "    state = env.reset()\n",
        "    state = np.reshape(state, [1, state_size])\n",
        "\n",
        "    while not done and step < n_max_steps:\n",
        "      action = get_action(state, action_size, model, epsilon)\n",
        "      next_state, reward, done, info = env.step(action)\n",
        "      next_state = np.reshape(next_state, [1, state_size])\n",
        "\n",
        "      memory.append(\n",
        "          (state, action, reward if not done else -100, next_state, done))\n",
        "      if epsilon > epsilon_min:\n",
        "        epsilon *= epsilon_decay  # Decrease randomness\n",
        "      train_replay(memory, batch_size, train_start, discount_factor, mm_omega, model)\n",
        "      score += reward\n",
        "      step += 1\n",
        "      state = next_state\n",
        "\n",
        "      if render:\n",
        "        env.render()\n",
        "\n",
        "      if done:\n",
        "        env.reset()\n",
        "        scores.append(score)\n",
        "        all_scores.append(score)\n",
        "        avg_score = np.mean(scores)\n",
        "\n",
        "        if len(solution) == 0 and avg_score >= n_win_ticks and e >= n_avg_scores:\n",
        "          # The start of a 100-episode averaging window with a mean score >= 195\n",
        "          solution_window_start = e - n_avg_scores\n",
        "\n",
        "          # The first episode score >= 195\n",
        "          solution_episode_idx = next(\n",
        "              x[0] for x in enumerate(all_scores) if x[1] >= n_win_ticks)\n",
        "\n",
        "          solution.append(solution_window_start)\n",
        "          print('Solved! Avg. reward >= 195.0 over 100 consecutive trials reached at episode {} \\o/'.format(\n",
        "              solution_window_start))\n",
        "          print('First score >= 195 reached at episode {}.'.format(\n",
        "              solution_episode_idx))\n",
        "\n",
        "        if verbose > 0 and e % logging_int == 0:\n",
        "          avg_display = '{:.2f}'.format(avg_score)\n",
        "          print('[Episode {}] Average Score: {} | Total Rewards: {:.2f}'.format(\n",
        "              e, avg_display, score))\n",
        "  return solution_window_start, np.mean(all_scores), all_scores\n",
        "\n",
        "def run_game(**config):\n",
        "  solution_window_start, avg_score, all_scores = learn_to_balance(**config)\n",
        "  if 'return_history' in config and config['return_history']:\n",
        "        return solution_window_start, avg_score, all_scores\n",
        "  if solution_window_start < 10:\n",
        "    return avg_score * config['win_10_scalar']\n",
        "  elif solution_window_start < 100:\n",
        "    return avg_score * config['win_100_scalar']\n",
        "  return avg_score"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t2Vux99O_zk5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 232
        },
        "outputId": "4c7ccc46-90d0-4fc5-a105-ce78e5977a4f"
      },
      "source": [
        "# Bounded region of parameter space\n",
        "SPACE = [skopt.space.Real(0.0005, 0.01, name='lr', prior='uniform'),\n",
        "         skopt.space.Real(0.9, 1.0, name='discount_factor', prior='uniform'),\n",
        "         skopt.space.Real(0.9, 0.99, name='epsilon_decay', prior='uniform'),\n",
        "         skopt.space.Real(0.9, 1.0, name='epsilon', prior='uniform'),\n",
        "         skopt.space.Real(1.0, 30.0, name='mm_omega', prior='uniform'),\n",
        "         skopt.space.Real(0.5, 1.5, name='model_layer_taper', prior='uniform'),\n",
        "         skopt.space.Integer(16, 128, name='model_layer_size'),\n",
        "         skopt.space.Integer(0, 2, name='model_activation_1'),\n",
        "         skopt.space.Integer(0, 2, name='model_activation_2'),\n",
        "         skopt.space.Integer(0, 2, name='model_activation_3'),\n",
        "         skopt.space.Integer(32, 128, name='batch_size'),\n",
        "         skopt.space.Integer(3, 5, name='model_layers')]\n",
        "\n",
        "@skopt.utils.use_named_args(SPACE)\n",
        "def objective(**params):\n",
        "    return -1.0 * run_game(**params, win_100_scalar=1.1, win_10_scalar=1.2)\n",
        "\n",
        "print('Searching parameter space... Now would be a good time to make coffee. ☕')\n",
        "\n",
        "results = skopt.gbrt_minimize(objective, SPACE, n_calls=500, callback=[skopt.callbacks.VerboseCallback(n_total=500)], random_state=123)\n",
        "print(results)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-b8db0af5cede>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m SPACE = [skopt.space.Real(0.0005, 0.01, name='lr', prior='uniform'),\n\u001b[0m\u001b[1;32m      2\u001b[0m          \u001b[0mskopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mReal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.9\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'discount_factor'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprior\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'uniform'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m          \u001b[0mskopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mReal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.9\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.99\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'epsilon_decay'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprior\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'uniform'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m          \u001b[0mskopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mReal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.9\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'epsilon'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprior\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'uniform'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m          \u001b[0mskopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mReal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m30.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'mm_omega'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprior\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'uniform'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'skopt' is not defined"
          ]
        }
      ]
    }
  ]
}